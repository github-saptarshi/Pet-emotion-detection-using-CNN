{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c496e6a-61b4-4db5-83de-f18eb6e4f9bd",
   "metadata": {},
   "source": [
    "# Pets Facial Expression classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0c4f13-d8f8-4827-8dd6-a4ea067d5498",
   "metadata": {},
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8052f30-7275-43f2-96f9-018234383a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce3373-7514-4ede-a18b-bc4178b9ed04",
   "metadata": {},
   "source": [
    "### Read The Image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "492975ec-1acd-403a-9db6-9db7772aa23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingImagePath=\"C:/Users/SAPTARSHIm/Downloads/Pet's Facial Expression Image Dataset/Master Folder/train\"\n",
    "TestingImagePath=\"C:/Users/SAPTARSHIm/Downloads/Pet's Facial Expression Image Dataset/Master Folder/test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f70ef45d-ff1e-4bf7-90eb-b94c6bc25955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 4 classes.\n",
      "Found 1000 images belonging to 4 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Angry': 0, 'Other': 1, 'Sad': 2, 'happy': 3}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# Defining pre-processing transformations on raw images of training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# Defining pre-processing transformations on raw images of testing data\n",
    "# test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "# Generating the Training Data\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(128, 128),\n",
    "        batch_size=16,\n",
    "        class_mode='categorical')\n",
    "\n",
    "\n",
    "# Generating the Testing Data\n",
    "# test_set = test_datagen.flow_from_directory(\n",
    "#         TrainingImagePath,\n",
    "#         target_size=(64, 64),\n",
    "#         batch_size=32,\n",
    "#         class_mode='categorical')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "valid_set = valid_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(128, 128),\n",
    "        batch_size=16,\n",
    "        class_mode='categorical')\n",
    "\n",
    "# Printing class labels for each face\n",
    "valid_set.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2b8a1-f9d6-42f3-a78c-69a6c877f222",
   "metadata": {},
   "source": [
    "### Creating a list of facial expression and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c6c0efd-37b0-4a0e-a404-60efb6acb1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of Face and its ID {0: 'Angry', 1: 'Other', 2: 'Sad', 3: 'happy'}\n"
     ]
    }
   ],
   "source": [
    "# class_indices have the numeric tag for each face\n",
    "TrainClasses=training_set.class_indices\n",
    "\n",
    "# Storing the face and the numeric tag for future reference\n",
    "ResultMap={}\n",
    "for x,emotion in zip(TrainClasses.values(),TrainClasses.keys()):\n",
    "    ResultMap[x]=emotion\n",
    "\n",
    "# # Saving the face map for future reference\n",
    "# import pickle\n",
    "# with open(\"ResultsMap.pkl\", 'wb') as f:\n",
    "#     pickle.dump(ResultMap, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"Mapping of Face and its ID\",ResultMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0a4e59-13f0-44bf-8e10-b0c9b3c6ffa8",
   "metadata": {},
   "source": [
    "## Creating the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a7a9e93-3ef1-490c-87dc-07bffd7501f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcdce6d9-f8f2-44f1-9f42-318a77e2718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# step 1\n",
    "model.add(Convolution2D(filters=32, kernel_size=(3,3), strides= (1, 1),input_shape=(128,128,3), activation='relu'))\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# step 2\n",
    "model.add(Convolution2D(filters=64, kernel_size=(3,3), strides= (1, 1), activation='relu'))\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# step 3\n",
    "\n",
    "model.add(Convolution2D(filters=64, kernel_size=(4,4), strides= (2, 2), activation='relu'))\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation='relu'))\n",
    "\n",
    "model.add(Dense(len(ResultMap),activation ='softmax'))\n",
    "#compile the layers\n",
    "model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1da6f4-6399-4e16-8bc1-ae0536d22266",
   "metadata": {},
   "source": [
    "## Fitting The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beb7c530-6436-49f6-a304-60018d9843be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "63/63 [==============================] - 20s 278ms/step - loss: 1.3905 - accuracy: 0.2490 - val_loss: 1.3698 - val_accuracy: 0.3125\n",
      "Epoch 2/50\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 1.3872 - accuracy: 0.2630 - val_loss: 1.3584 - val_accuracy: 0.2812\n",
      "Epoch 3/50\n",
      "63/63 [==============================] - 18s 274ms/step - loss: 1.3779 - accuracy: 0.2940 - val_loss: 1.3593 - val_accuracy: 0.2812\n",
      "Epoch 4/50\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 1.3204 - accuracy: 0.3680 - val_loss: 1.2781 - val_accuracy: 0.4062\n",
      "Epoch 5/50\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 1.2823 - accuracy: 0.4000 - val_loss: 1.0489 - val_accuracy: 0.6250\n",
      "Epoch 6/50\n",
      "63/63 [==============================] - 17s 274ms/step - loss: 1.2328 - accuracy: 0.4460 - val_loss: 1.1363 - val_accuracy: 0.5625\n",
      "Epoch 7/50\n",
      "63/63 [==============================] - 18s 278ms/step - loss: 1.1688 - accuracy: 0.4870 - val_loss: 0.9677 - val_accuracy: 0.5625\n",
      "Epoch 8/50\n",
      "63/63 [==============================] - 19s 290ms/step - loss: 1.0800 - accuracy: 0.5260 - val_loss: 1.0777 - val_accuracy: 0.5312\n",
      "Epoch 9/50\n",
      "63/63 [==============================] - 18s 275ms/step - loss: 1.0466 - accuracy: 0.5310 - val_loss: 0.9048 - val_accuracy: 0.6875\n",
      "Epoch 10/50\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.9306 - accuracy: 0.6070 - val_loss: 0.6270 - val_accuracy: 0.7500\n",
      "Epoch 11/50\n",
      "63/63 [==============================] - 18s 278ms/step - loss: 0.8603 - accuracy: 0.6510 - val_loss: 0.7318 - val_accuracy: 0.7188\n",
      "Epoch 12/50\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 0.7408 - accuracy: 0.7030 - val_loss: 0.5840 - val_accuracy: 0.7500\n",
      "Epoch 13/50\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.7381 - accuracy: 0.7100 - val_loss: 0.7314 - val_accuracy: 0.6250\n",
      "Epoch 14/50\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.6272 - accuracy: 0.7490 - val_loss: 0.7424 - val_accuracy: 0.6875\n",
      "Epoch 15/50\n",
      "63/63 [==============================] - 19s 293ms/step - loss: 0.5499 - accuracy: 0.7950 - val_loss: 0.5474 - val_accuracy: 0.8750\n",
      "Epoch 16/50\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 0.4352 - accuracy: 0.8390 - val_loss: 0.4919 - val_accuracy: 0.8125\n",
      "Epoch 17/50\n",
      "63/63 [==============================] - 18s 288ms/step - loss: 0.3744 - accuracy: 0.8630 - val_loss: 0.3466 - val_accuracy: 0.8438\n",
      "Epoch 18/50\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.3286 - accuracy: 0.8920 - val_loss: 0.2627 - val_accuracy: 0.9375\n",
      "Epoch 19/50\n",
      "63/63 [==============================] - 18s 275ms/step - loss: 0.3366 - accuracy: 0.8880 - val_loss: 0.1773 - val_accuracy: 0.9375\n",
      "Epoch 20/50\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.2706 - accuracy: 0.9160 - val_loss: 0.3304 - val_accuracy: 0.9375\n",
      "Epoch 21/50\n",
      "63/63 [==============================] - 17s 276ms/step - loss: 0.2344 - accuracy: 0.9170 - val_loss: 0.0820 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1732 - accuracy: 0.9490 - val_loss: 0.3470 - val_accuracy: 0.9062\n",
      "Epoch 23/50\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1727 - accuracy: 0.9440 - val_loss: 0.0906 - val_accuracy: 0.9688\n",
      "Epoch 24/50\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.1501 - accuracy: 0.9480 - val_loss: 0.3131 - val_accuracy: 0.9062\n",
      "Epoch 25/50\n",
      "63/63 [==============================] - 18s 276ms/step - loss: 0.1395 - accuracy: 0.9540 - val_loss: 0.1049 - val_accuracy: 0.9688\n",
      "Epoch 26/50\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.1031 - accuracy: 0.9660 - val_loss: 0.1060 - val_accuracy: 0.9688\n",
      "Epoch 27/50\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.1022 - accuracy: 0.9670 - val_loss: 0.0276 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 0.1566 - accuracy: 0.9510 - val_loss: 0.0435 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "63/63 [==============================] - 18s 282ms/step - loss: 0.1306 - accuracy: 0.9570 - val_loss: 0.1031 - val_accuracy: 0.9688\n",
      "Epoch 30/50\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1091 - accuracy: 0.9640 - val_loss: 0.0115 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "63/63 [==============================] - 18s 293ms/step - loss: 0.0554 - accuracy: 0.9840 - val_loss: 0.0272 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0728 - accuracy: 0.9740 - val_loss: 0.2145 - val_accuracy: 0.9375\n",
      "Epoch 33/50\n",
      "63/63 [==============================] - 19s 294ms/step - loss: 0.0601 - accuracy: 0.9840 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0617 - accuracy: 0.9760 - val_loss: 0.0380 - val_accuracy: 0.9688\n",
      "Epoch 35/50\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.0366 - accuracy: 0.9890 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0379 - accuracy: 0.9890 - val_loss: 0.0121 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 0.0464 - accuracy: 0.9850 - val_loss: 0.0688 - val_accuracy: 0.9688\n",
      "Epoch 38/50\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 0.1301 - accuracy: 0.9610 - val_loss: 0.0669 - val_accuracy: 0.9688\n",
      "Epoch 39/50\n",
      "63/63 [==============================] - 18s 288ms/step - loss: 0.0524 - accuracy: 0.9830 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 0.0425 - accuracy: 0.9860 - val_loss: 0.0485 - val_accuracy: 0.9688\n",
      "Epoch 41/50\n",
      "63/63 [==============================] - 18s 281ms/step - loss: 0.1133 - accuracy: 0.9690 - val_loss: 0.0450 - val_accuracy: 0.9688\n",
      "Epoch 42/50\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.0814 - accuracy: 0.9760 - val_loss: 0.0795 - val_accuracy: 0.9688\n",
      "Epoch 43/50\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0326 - accuracy: 0.9910 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "63/63 [==============================] - 19s 301ms/step - loss: 0.0922 - accuracy: 0.9680 - val_loss: 0.1644 - val_accuracy: 0.9062\n",
      "Epoch 45/50\n",
      "63/63 [==============================] - 17s 266ms/step - loss: 0.0864 - accuracy: 0.9720 - val_loss: 0.0218 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "63/63 [==============================] - 18s 278ms/step - loss: 0.0180 - accuracy: 0.9960 - val_loss: 6.6158e-04 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 0.0171 - accuracy: 0.9960 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "63/63 [==============================] - 19s 291ms/step - loss: 0.0155 - accuracy: 0.9960 - val_loss: 0.0146 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.0074 - accuracy: 0.9990 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.0134 - accuracy: 0.9960 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "############### Total Time Taken:  15 Minutes #############\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Measuring the time taken by the model to train\n",
    "StartTime=time.time()\n",
    "\n",
    "# Starting the model training\n",
    "model.fit(\n",
    "                    training_set,\n",
    "                    epochs=50,\n",
    "                    validation_data=valid_set,\n",
    "                    validation_steps=2)\n",
    "\n",
    "EndTime=time.time()\n",
    "print(\"############### Total Time Taken: \", round((EndTime-StartTime)/60), 'Minutes #############')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed5f16d-74bb-4639-93ac-4c1608827cc3",
   "metadata": {},
   "source": [
    "## saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56dab225-df42-4a9f-a7f0-bbd82e09c82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.pkl\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.pkl\\assets\n"
     ]
    }
   ],
   "source": [
    "## Saving the model\n",
    "model.save(\"model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9134d849-b62b-4ba6-8188-7e623c2753af",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing the model on a different face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7c2b6b55-a479-448e-aa17-b124cc4855f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1.]]\n",
      "{0: 'Angry', 1: 'Other', 2: 'Sad', 3: 'happy'}\n",
      "########################################\n",
      "Prediction is:  happy\n"
     ]
    }
   ],
   "source": [
    "'''########################## Making single predictions ############################'''\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "testImage=\"C:/Users/SAPTARSHIm/Downloads/Pet's Facial Expression Image Dataset/Master Folder/test/happy/105.jpg\"\n",
    "test_image=image.load_img(testImage,target_size=(128, 128))\n",
    "test_image=image.img_to_array(test_image)\n",
    "\n",
    "test_image_1=np.expand_dims(test_image,axis=0)\n",
    "\n",
    "result=model.predict(test_image_1,verbose=0)\n",
    "print(result)\n",
    "print(ResultMap)\n",
    "\n",
    "print('####'*10)\n",
    "print('Prediction is: ',ResultMap[np.argmax(result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c751e92f-2711-4eb6-9c13-784a80291d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9708e50a-307e-4b34-b602-6aec3a85cebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
